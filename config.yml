input_data:
  read_from_wikicaps_datasource: True # if True the data is read from the raw wikicaps file otherwise from metadata_dataframe
  wikicaps_datasource: ./data/wikicaps_data_list_unfiltered_100
  load_from_dataframe: False # TODO
  metadata_dataframe: ./wikicaps_etl_out/metadata.df.feather # TODO
  shuffle: True
  random_seed: 1312
  pos_tag_stats: False
  max_samples: 1000 # after filtering
  filters:
    - NumTokens:
        columnId: num_tok
        max: 150
        min: 10

    - MinSentenceLength:
        columnId: min_sent_len
        min: 5

    - NumSents:
        columnId: num_sent
        max: 5
        min: 1


output:
  log_file: ./wikicaps_etl_out/full.log
  img_format: png # choice of: png, jpg, npy, npz
  img_directory: ./wikicaps_etl_out/images/
  metadata_file: ./wikicaps_etl_out/metadata.df # file extension doesn't matter

download:
  with_skimage: False # otherwise with python requests and pillow
  n_workers: 8
  max_img_width: 500 # in px

spacy:
  model: en_core_web_lg
  use_gpu: True # Maybe it's way faster not using GPU but 50 CPUs
  n_workers: 6 # when using GPU, all workers allocate memory on a single GPU


